# Library for loading PDF files and extracting text from each page
# - Used to convert text-based PDF documents into raw text
pypdf==5.1.0

# Library for generating semantic embeddings from sentences or text chunks
# - Runs locally and is free to use
# - Used for document and query embeddings in the RAG pipeline
# - "all-MiniLM-L6-v2" was used, which is a lightweight and efficient sentence embedding model
sentence-transformers==2.7.0

# High-performance vector similarity search library by Facebook AI Research
# - Stores embedding vectors and performs fast nearest-neighbor search
# - Core component for the Retrieval step in RAG
faiss-cpu>=1.9.0

#  Python client library for interacting with a local Ollama server
# - Used to send prompts and receive responses from locally running LLMs
# - Enables LLM-based answer generation without using paid cloud APIs
# - During development and testing, the local model "llama3.2:3b" was used
#   (a lightweight 3B-parameter LLaMA-based model suitable for RAG pipelines)
ollama==0.4.4


# Official Python client for OpenAI API (ChatGPT)
# - Used to compare responses from a commercial LLM against a local LLM
# - Enables evaluation of answer quality, inference behavior, and hallucination risk
# - API usage is intentionally limited to small-scale evaluation (Phase 1.5)
openai>=1.30.0

# Google Generative AI Python SDK (Gemini)
# - Used to compare Google's Gemini models with ChatGPT and local LLMs
# - Provides a free-tier API suitable for lightweight experiments
# - Helps analyze differences in grounding and inference across LLM providers
# google-generativeai>=0.7.0
# Google Gen AI SDK (Gemini) - current official package (replaces google-generativeai)
google-genai>=0.6.0


# Streamlit is used to build a lightweight web UI for demonstrating
# the end-to-end RAG pipeline (PDF upload, retrieval, and answer generation).
streamlit==1.52.2